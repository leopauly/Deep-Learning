
 LeedsNet < New Attempts No.1> : Results when LeedsNet was trained on humanvsanimal dataset 


 Network architechture 

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
convolution2d_1 (Convolution2D)  (None, 30, 30, 16)    448         convolution2d_input_1[0][0]      
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 30, 30, 16)    0           convolution2d_1[0][0]            
____________________________________________________________________________________________________
convolution2d_2 (Convolution2D)  (None, 28, 28, 32)    4640        dropout_1[0][0]                  
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 28, 28, 32)    0           convolution2d_2[0][0]            
____________________________________________________________________________________________________
maxpooling2d_1 (MaxPooling2D)    (None, 14, 14, 32)    0           dropout_2[0][0]                  
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 6272)          0           maxpooling2d_1[0][0]             
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 256)           1605888     flatten_1[0][0]                  
____________________________________________________________________________________________________
dropout_3 (Dropout)              (None, 256)           0           dense_1[0][0]                    
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 2)             514         dropout_3[0][0]                  
====================================================================================================
Total params: 1611490
____________________________________________________________________________________________________
None

 Results: Colomn headings 
 dict_keys(['val_loss', 'val_acc', 'acc', 'loss'])

 Results: Training 
 {'val_loss': [0.1797073442440737, 0.081749524027992293, 0.057017457903290554, 0.049696920362325259, 0.058775038071526696, 0.038014570802198284, 0.036159187172196308, 0.030660420304574755, 0.0313353599875223, 0.027662704665027408, 0.027867915266989288, 0.025241551253798365, 0.020310680250735583, 0.02137210208968162, 0.025998439151909882, 0.018910121679615761, 0.020047583759472624, 0.020362000215849045, 0.021653362763980364, 0.021329574121315593, 0.016724080336271544, 0.016557057400726487, 0.014490014443248159, 0.017745359197631622, 0.017192188834436475, 0.014342036727402406, 0.01651718023966733, 0.013832246531447269, 0.017268029839434693, 0.014461139566578666, 0.014340576139418238, 0.014202734196728749, 0.014336056128606625, 0.012050362122085422, 0.012848625626722015, 0.015235794448170149, 0.012282774178894658, 0.012963628453082398, 0.01225031143973977, 0.010817282123703609, 0.011201559729052971, 0.011326711294537899, 0.012787071007251133, 0.01560378755844199, 0.010939339322955375, 0.011458204360094922, 0.011028503356517562, 0.012400959757478347, 0.013135007716831798, 0.0098614757323900234], 'val_acc': [0.95770474491191326, 0.98518319203165072, 0.98733836470236036, 0.99003232976999778, 0.9859913816868231, 0.99164870888765511, 0.99137931220747277, 0.9943426737626051, 0.99272629464494766, 0.99595905268757512, 0.99515086322509005, 0.99488146673759514, 0.99649784566256505, 0.99676724215006007, 0.99461207025010012, 0.99622844917507003, 0.99595905268757512, 0.99649784566256505, 0.99703663863755509, 0.9956896562000801, 0.99703663863755509, 0.99649784566256505, 0.99730603512505001, 0.9956896562000801, 0.99542025971258508, 0.99784482810004005, 0.99622844917507003, 0.99784482810004005, 0.9956896562000801, 0.99649784566256505, 0.99676724215006007, 0.99676724215006007, 0.99595905268757512, 0.99730603512505001, 0.99757543161254503, 0.9956896562000801, 0.99703663863755509, 0.99730603512505001, 0.99622844917507003, 0.99730603512505001, 0.99703663863755509, 0.99757543161254503, 0.99622844917507003, 0.99595905268757512, 0.99838362107502998, 0.99757543161254503, 0.99811422458753507, 0.99622844917507003, 0.9956896562000801, 0.99811422458753507], 'acc': [0.87762661925356444, 0.96605603925177252, 0.97710129711776972, 0.98033405525674078, 0.9853852397826468, 0.9854525890008643, 0.98848329982238592, 0.98868534713983536, 0.99023437699110339, 0.99205280352255398, 0.99090786835435651, 0.99198545435250829, 0.99353448420377644, 0.99238954918009459, 0.99366918235118018, 0.99353448425194824, 0.99339978596002887, 0.99488146664125143, 0.99535291059071129, 0.99494881581129702, 0.99508351410321638, 0.99582435434748384, 0.99528556146883762, 0.99595905259123141, 0.99575700532195388, 0.99649784566256505, 0.99676724215006007, 0.99690194034563573, 0.99750808249067124, 0.99757543161254503, 0.99730603507687821, 0.99643049649251947, 0.99757543161254503, 0.99669989302818629, 0.99771012985629259, 0.9979795263437875, 0.99851831931877755, 0.99777747897816627, 0.99750808244249944, 0.99885506492814624, 0.998653017562525, 0.99845097019690376, 0.99683459112741823, 0.99838362107502998, 0.99885506492814624, 0.99784482810004005, 0.99804687541748938, 0.99851831927060564, 0.99892241405002002, 0.99905711229376748], 'loss': [0.28191102500292392, 0.091878691010543689, 0.064876015336267301, 0.053890411144462191, 0.042339326099874466, 0.038361149205661797, 0.033073596160966875, 0.032664172101368842, 0.029257101325490684, 0.025322202185237723, 0.026263174883227741, 0.022036003444280944, 0.019366674944452652, 0.019682824058188761, 0.018897019408586953, 0.018320601144299876, 0.017190372189526248, 0.016478809793859663, 0.015103822782588013, 0.013560775089528637, 0.014330965164840195, 0.01229885255854588, 0.012576344516126789, 0.010910811990743109, 0.01175954528315506, 0.0098978467618528326, 0.011380613107333943, 0.0085603274548901771, 0.0078769117627609329, 0.007522034684992903, 0.0083428756881515232, 0.0081695183630513413, 0.0068691606721587792, 0.008933439862024585, 0.0065749172903901429, 0.0063152177959529526, 0.0058323776325224386, 0.0062939229194731256, 0.0071832498822153763, 0.0042025741608641334, 0.0042098732094844353, 0.0048231943753312741, 0.0079086736650073527, 0.0041277966218129054, 0.0041171601308506592, 0.0054102345762751195, 0.0047064414890147363, 0.0045484999112848932, 0.0041218607411306375, 0.0031351421805414391]} 


 Time taken for training 
  Start Time: 2017-02-09 14:01:58.593450 
 End Time: 2017-02-09 14:14:55.745499 


 Accuracy on testing data: 99.42% 


 LeedsNet < New Attempts No.2> : Results when LeedsNet was trained on humanvsanimal dataset 


 Network architechture 

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
convolution2d_1 (Convolution2D)  (None, 30, 30, 16)    448         convolution2d_input_1[0][0]      
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 30, 30, 16)    0           convolution2d_1[0][0]            
____________________________________________________________________________________________________
convolution2d_2 (Convolution2D)  (None, 28, 28, 32)    4640        dropout_1[0][0]                  
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 28, 28, 32)    0           convolution2d_2[0][0]            
____________________________________________________________________________________________________
maxpooling2d_1 (MaxPooling2D)    (None, 14, 14, 32)    0           dropout_2[0][0]                  
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 6272)          0           maxpooling2d_1[0][0]             
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 256)           1605888     flatten_1[0][0]                  
____________________________________________________________________________________________________
dropout_3 (Dropout)              (None, 256)           0           dense_1[0][0]                    
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 2)             514         dropout_3[0][0]                  
====================================================================================================
Total params: 1611490
____________________________________________________________________________________________________
None

 Results: Colomn headings 
 dict_keys(['val_loss', 'acc', 'val_acc', 'loss'])

 Results: Training 
 {'val_loss': [0.17528150290459524, 0.08513513975776732, 0.054935877936478736, 0.049557640332656368, 0.058025052845738986, 0.038547201910942339, 0.036062699909585903, 0.032244669553243697, 0.030552777601895224, 0.026926910059704948, 0.02699551346136261, 0.024053239972153247, 0.02044567454368007, 0.020421322260188328, 0.026539245339147876, 0.018982263925267785, 0.019595657433118786, 0.020733273804674761, 0.022843639241351601, 0.020872167119219302, 0.017049913749125498, 0.016400123635940903, 0.015284967971056097, 0.017622765943887599, 0.016696275786886934, 0.014253106291442722, 0.015610203166786584, 0.013671115374590131, 0.014476224883412277, 0.01319131365078202, 0.013407554982583452, 0.014438346059887801, 0.014687298815649957, 0.012002687322857081, 0.013153450277574676, 0.013835839119231988, 0.019398253725698617, 0.012341351657075141, 0.011662140124025003, 0.010754090261397077, 0.011275781705770061, 0.011237092225875475, 0.0114819912855457, 0.017309997228860029, 0.010355317766904168, 0.012199731881171884, 0.01139314471543282, 0.013603979825382998, 0.011195087121270869, 0.010243183928304068], 'acc': [0.87722252423329083, 0.96477640574348389, 0.97730334443521916, 0.98033405511222527, 0.98565463622196992, 0.98525054168341486, 0.98807920504297164, 0.98801185592109786, 0.99050377362311404, 0.99158135957309401, 0.99104256654993217, 0.99212015254808394, 0.99333243678998329, 0.9926589456675895, 0.99306304015797275, 0.99313038937619025, 0.99427532459255952, 0.99420797542251393, 0.99542025971258508, 0.99542025971258508, 0.99454472112822634, 0.99562230707820631, 0.99481411756754945, 0.99568965615190819, 0.99555495795633253, 0.99609375088315066, 0.99622844917507003, 0.99737338419875199, 0.99710398771125697, 0.9976427806862469, 0.99737338424692379, 0.99710398775942877, 0.9979795263437875, 0.99629579829694381, 0.99764278073441881, 0.99777747897816627, 0.99831627195315631, 0.99804687546566129, 0.99744073336879757, 0.99878771580627257, 0.99858566844065133, 0.99818157370940874, 0.99737338410240828, 0.99831627195315631, 0.99858566844065133, 0.99804687546566129, 0.99858566839247942, 0.99878771575810066, 0.99885506492814624, 0.99912446141564126], 'val_acc': [0.95824353827227804, 0.98437500256916577, 0.98760776118985538, 0.9903017262574928, 0.98679957114930805, 0.9913793124001602, 0.99110991591266517, 0.99380388078761517, 0.99299569113244268, 0.99595905268757512, 0.99542025971258508, 0.9956896562000801, 0.99622844917507003, 0.99676724215006007, 0.9943426737626051, 0.99595905268757512, 0.99622844917507003, 0.99595905268757512, 0.99676724215006007, 0.99595905268757512, 0.99703663863755509, 0.99649784566256505, 0.99676724215006007, 0.9956896562000801, 0.99676724215006007, 0.99757543161254503, 0.99622844917507003, 0.99784482810004005, 0.99784482810004005, 0.99730603512505001, 0.99703663863755509, 0.99649784566256505, 0.99595905268757512, 0.99757543161254503, 0.99703663863755509, 0.99676724215006007, 0.99488146673759514, 0.99784482810004005, 0.99730603512505001, 0.99757543161254503, 0.99730603512505001, 0.99784482810004005, 0.99703663863755509, 0.99515086322509005, 0.99838362107502998, 0.99730603512505001, 0.99757543161254503, 0.9956896562000801, 0.99730603512505001, 0.99784482810004005], 'loss': [0.28137670360944145, 0.092379095011892787, 0.064706755475997163, 0.054353397616868561, 0.041909582211138934, 0.037928645300228998, 0.033058194001876311, 0.032933781520519047, 0.028445491192854888, 0.025353916278090142, 0.026472850161223128, 0.021239222076810695, 0.019399896092685241, 0.019842119932415723, 0.01861755657964647, 0.018370365468424302, 0.016587357704202547, 0.016331490677612467, 0.014857040104179056, 0.013348300011338056, 0.014052292242051309, 0.011731291953220547, 0.012383885947184118, 0.010941253594970116, 0.0115338868617557, 0.010124106142179642, 0.010930236900981313, 0.0082866861868584763, 0.0083098263166158943, 0.0074095897036299992, 0.0081466728653521499, 0.0076695568344265783, 0.0064890755495704668, 0.0089391930500501218, 0.0063807270928478519, 0.0059924293215614481, 0.0052894928123992293, 0.0063596610156413606, 0.0064378234777764352, 0.003881416256935266, 0.004070911112330002, 0.0055722808374381845, 0.0065580511270445663, 0.0043419208719782778, 0.004228606059763323, 0.0053405126092537069, 0.0041454383071926745, 0.0038634154185441423, 0.0038419589992793927, 0.0027284181928347927]} 


 Time taken for training 
  Start Time: 2017-02-09 17:35:47.122040 
 End Time: 2017-02-09 17:49:04.719437 


 Accuracy on testing data: 99.42% 

[[  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  1.25248867e-29   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  1.30660626e-30   1.00000000e+00]
 [  1.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  1.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  1.00000000e+00   1.30624519e-26]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  7.49498188e-01   2.50501812e-01]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  1.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  1.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  1.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  8.39166403e-01   1.60833627e-01]
 [  1.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  1.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  1.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  1.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  1.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  1.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  1.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  1.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  1.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  1.00000000e+00   0.00000000e+00]
 [  1.00000000e+00   0.00000000e+00]
 [  5.17634419e-07   9.99999523e-01]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  1.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  1.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  2.58971406e-19   1.00000000e+00]
 [  1.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  1.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  1.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  1.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]
 [  0.00000000e+00   1.00000000e+00]]

 LeedsNet < New Attempts No.1> : Results when LeedsNet was trained on humanvsanimal dataset 


 Network architechture 

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
convolution2d_1 (Convolution2D)  (None, 30, 30, 16)    448         convolution2d_input_1[0][0]      
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 30, 30, 16)    0           convolution2d_1[0][0]            
____________________________________________________________________________________________________
convolution2d_2 (Convolution2D)  (None, 28, 28, 32)    4640        dropout_1[0][0]                  
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 28, 28, 32)    0           convolution2d_2[0][0]            
____________________________________________________________________________________________________
maxpooling2d_1 (MaxPooling2D)    (None, 14, 14, 32)    0           dropout_2[0][0]                  
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 6272)          0           maxpooling2d_1[0][0]             
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 256)           1605888     flatten_1[0][0]                  
____________________________________________________________________________________________________
dropout_3 (Dropout)              (None, 256)           0           dense_1[0][0]                    
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 2)             514         dropout_3[0][0]                  
====================================================================================================
Total params: 1611490
____________________________________________________________________________________________________
None

 Results: Colomn headings 
 dict_keys(['val_loss', 'acc', 'val_acc', 'loss'])

 Results: Training 
 {'val_loss': [6.0527255868911745, 6.139409952163696, 6.3855414819717407, 6.4979758834838863, 6.57981739932191, 6.9463952851295474, 6.471866550445557, 6.9605014777183536, 6.9022087204786295, 6.8801335129141812, 6.6635511541366581, 6.6649074551090601, 6.9390536022186282, 6.8159215534536637, 6.6129331898689268, 6.7054379106582926, 6.8170021390914917, 6.5242295900980691, 6.7340557714303344, 6.5174157883497541, 6.5791960415302313, 6.725449056625366, 6.6194884777069092, 6.470990952018183, 6.4472382974624631, 6.5435243844985962, 6.5746342945098881, 6.5424841356277463, 6.4079771232604976, 6.4473299011052587, 6.3402421147748829, 6.4647730493545534, 6.7556932163238521, 6.6527017593383793, 6.7344420862197873, 6.5009651803970341, 6.7160055303573607, 6.4491039419174196, 6.593192582130432, 6.641547999382019, 6.6340574502944945, 6.3935113143920894, 6.5848957586288455, 6.531479742228985, 6.5102947974299425, 6.4636464774608733, 6.5486615716401868, 6.3986114707856903, 6.4472383022308346, 6.5009652423858642], 'acc': [0.89418103793828652, 0.97332974527159644, 0.9806573310417348, 0.98518319264825049, 0.98679957172737043, 0.98852370909318843, 0.99014008832645828, 0.99197198447996171, 0.99143319154350917, 0.99245689823452765, 0.99202586393161063, 0.9929418119890937, 0.99369612215407965, 0.99353448418450763, 0.99466594954759913, 0.99488146662198262, 0.99525862178155056, 0.9956896562000801, 0.99633620769299314, 0.99612069054153463, 0.99671336285256107, 0.99660560421902555, 0.99622844917507003, 0.99676724211152257, 0.99612069050299712, 0.99660560421902555, 0.997252155827551, 0.99671336285256107, 0.99789870739753905, 0.99811422458753507, 0.99789870735900155, 0.99752155231504602, 0.99773706950504204, 0.99773706950504204, 0.99827586248003208, 0.99779094880254104, 0.99827586248003208, 0.99854525896752699, 0.99827586248003208, 0.99827586248003208, 0.99816810388503407, 0.9987068968214865, 0.99849137967002799, 0.99773706950504204, 0.99924568983501405, 0.99870689686002401, 0.99870689686002401, 0.99870689686002401, 0.9987068968214865, 0.99897629330898152], 'val_acc': [0.60999999999999999, 0.61666666656732561, 0.60333333492279051, 0.59333333522081377, 0.58333333253860475, 0.5600000032782555, 0.59333333432674407, 0.5666666662693024, 0.57000000149011609, 0.57000000476837154, 0.58333333373069762, 0.58666666865348815, 0.56666667014360428, 0.57000000119209293, 0.58666666954755786, 0.58000000357627868, 0.57333333462476732, 0.59000000208616254, 0.58000000447034838, 0.5933333349227905, 0.58666666686534885, 0.57999999493360521, 0.58000000059604639, 0.59666666626930231, 0.60000000059604641, 0.59333333343267436, 0.58999999821186067, 0.59333333551883694, 0.59999999999999998, 0.60000000149011612, 0.60666666597127916, 0.59666666656732559, 0.58000000029802323, 0.58666666865348815, 0.58000000238418581, 0.59666666895151144, 0.57666666656732557, 0.59999999970197682, 0.58666666835546488, 0.58666666567325587, 0.58666666746139529, 0.60333333253860477, 0.58666667044162746, 0.59000000119209295, 0.59333333551883694, 0.59666666567325588, 0.59333333581686021, 0.60000000029802325, 0.60000000119209285, 0.59666666716337202], 'loss': [0.24558057438447703, 0.074473543650072249, 0.055201553906806956, 0.042314399393844411, 0.036360555210576267, 0.030941737350706419, 0.028532230675175921, 0.02374782730767655, 0.023290605592397345, 0.021032894965621697, 0.020636402316790491, 0.019745293237243396, 0.01705348494585664, 0.016494630023144514, 0.01521385629064151, 0.01486029841667943, 0.013406196078908932, 0.012540001572199954, 0.01048114279123844, 0.011260592890534306, 0.0095106040257703518, 0.0095123108757730456, 0.010316291003438751, 0.0092730584252786429, 0.0095518071123868349, 0.0085054701635929476, 0.00808514833187895, 0.0083221184589373446, 0.0067693460603905405, 0.0061761572748405122, 0.0054891889742319641, 0.0070900465150820135, 0.0066993803909901643, 0.0067791893520061175, 0.0047108846675323456, 0.0057111687607832931, 0.0045762512748689218, 0.00448359086701401, 0.005019544512896594, 0.0048346030693647421, 0.0054023606452804317, 0.0038419875095420516, 0.0045000617848938718, 0.0054975088309701025, 0.0026049745809775568, 0.0034307001038060268, 0.0035586300009256551, 0.0033678207548851948, 0.0039321473573550774, 0.003081908025942473]} 


 Time taken for training 
  Start Time: 2017-02-09 18:15:21.497002 
 End Time: 2017-02-09 18:31:19.433708 

#Evaluated on images taken from same dataset
  32/2063 [..............................] - ETA: 0s
  96/2063 [>.............................] - ETA: 0s
 160/2063 [=>............................] - ETA: 0s
 224/2063 [==>...........................] - ETA: 0s
 288/2063 [===>..........................] - ETA: 0s
 352/2063 [====>.........................] - ETA: 0s
 416/2063 [=====>........................] - ETA: 0s
 480/2063 [=====>........................] - ETA: 0s
 544/2063 [======>.......................] - ETA: 0s
 608/2063 [=======>......................] - ETA: 0s
 672/2063 [========>.....................] - ETA: 0s
 736/2063 [=========>....................] - ETA: 0s
 800/2063 [==========>...................] - ETA: 0s
 864/2063 [===========>..................] - ETA: 0s
 928/2063 [============>.................] - ETA: 0s
 992/2063 [=============>................] - ETA: 0s
1056/2063 [==============>...............] - ETA: 0s
1120/2063 [===============>..............] - ETA: 0s
1184/2063 [================>.............] - ETA: 0s
1248/2063 [=================>............] - ETA: 0s
1312/2063 [==================>...........] - ETA: 0s
1376/2063 [===================>..........] - ETA: 0s
1440/2063 [===================>..........] - ETA: 0s
1504/2063 [====================>.........] - ETA: 0s
1568/2063 [=====================>........] - ETA: 0s
1632/2063 [======================>.......] - ETA: 0s
1696/2063 [=======================>......] - ETA: 0s
1760/2063 [========================>.....] - ETA: 0s
1824/2063 [=========================>....] - ETA: 0s
1888/2063 [==========================>...] - ETA: 0s
1952/2063 [===========================>..] - ETA: 0s
2016/2063 [============================>.] - ETA: 0sAccuracy: 99.56%

#Evaluated on images taken from different dataset dataset
 32/300 [==>...........................] - ETA: 0s
 96/300 [========>.....................] - ETA: 0s
160/300 [===============>..............] - ETA: 0s
224/300 [=====================>........] - ETA: 0s
288/300 [===========================>..] - ETA: 0sAccuracy: 59.67%
